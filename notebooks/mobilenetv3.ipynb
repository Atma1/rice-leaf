{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d478f01c",
   "metadata": {},
   "source": [
    "# Multi-Scenario MobileNetV3 Transfer Learning\n",
    "\n",
    "## Experiment Overview\n",
    "This notebook implements a comprehensive transfer learning experiment using **MobileNetV3Large** for rice leaf disease classification.\n",
    "\n",
    "### Dataset\n",
    "- **Original Classes**: 10 rice leaf disease categories\n",
    "- **Filtered Classes**: 4 selected classes\n",
    "  - bacterial_leaf_blight\n",
    "  - brown_spot\n",
    "  - leaf_blast\n",
    "  - healthy\n",
    "\n",
    "### Experiment Design\n",
    "**18 Training Scenarios** combining:\n",
    "- **3 Data Splits**: 90:10, 80:20, 70:30 (train:validation)\n",
    "- **2 Optimizers**: Adam, SGD\n",
    "- **3 Learning Rates**: 0.1, 0.01, 0.001\n",
    "- **3 Epoch Settings**: 15, 30, 45\n",
    "\n",
    "### Training Configuration\n",
    "| Scenario | Split Ratio | Optimizer | LR | Epochs |\n",
    "|----------|-------------|-----------|-----|--------|\n",
    "| 1-2 | 90:10 | Adam, SGD | 0.1 | 15 |\n",
    "| 3-4 | 90:10 | Adam, SGD | 0.01 | 30 |\n",
    "| 5-6 | 90:10 | Adam, SGD | 0.001 | 45 |\n",
    "| 7-8 | 80:20 | Adam, SGD | 0.1 | 15 |\n",
    "| 9-10 | 80:20 | Adam, SGD | 0.01 | 30 |\n",
    "| 11-12 | 80:20 | Adam, SGD | 0.001 | 45 |\n",
    "| 13-14 | 70:30 | Adam, SGD | 0.1 | 15 |\n",
    "| 15-16 | 70:30 | Adam, SGD | 0.01 | 30 |\n",
    "| 17-18 | 70:30 | Adam, SGD | 0.001 | 45 |\n",
    "\n",
    "### Method\n",
    "- **Transfer Learning**: Feature extraction with frozen MobileNetV3Large base\n",
    "- **Preprocessing**: Resize to 224×224, rescale to [0,1]\n",
    "- **Early Stopping**: Patience=5 epochs on validation loss\n",
    "- **Architecture**: MobileNetV3Large → GlobalAvgPool → Dropout(0.5) → Dense(4)\n",
    "\n",
    "### Outputs\n",
    "1. CSV file with all scenario results\n",
    "2. JSON file with training histories\n",
    "3. Top 10 scenarios bar chart\n",
    "4. Top 3 training curves\n",
    "5. Confusion matrix for best scenario\n",
    "6. Classification report for best scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f68553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Protobuf configured successfully\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshutil\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Reinstall protobuf with correct version\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                      \"--quiet\", \"--force-reinstall\", \"protobuf==3.20.3\"])\n",
    "\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "print(\"✓ Protobuf configured successfully\")\n",
    "\n",
    "# =====================================================\n",
    "# MAIN TRAINING CODE\n",
    "# =====================================================\n",
    "import shutil\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ------------------ 1. CONFIGURATION ------------------\n",
    "class Config:\n",
    "    BASE_INPUT = '/kaggle/input/rice-leaf-diseases-detection/Rice_Leaf_Diease/Rice_Leaf_Diease'\n",
    "    WORK_DIR = '/kaggle/working/Rice_Leaf_Diease'\n",
    "    OUTPUT_DIR = '/kaggle/working/results'\n",
    "    \n",
    "    # Target classes for filtering (4 out of 10 classes)\n",
    "    TARGET_CLASSES = ['bacterial_leaf_blight', 'brown_spot', 'leaf_blast', 'healthy']\n",
    "    \n",
    "    BATCH_SIZE = 16\n",
    "    IMG_SIZE_MOBILE = 224\n",
    "    \n",
    "    # Model architecture\n",
    "    DROPOUT_RATE = 0.5\n",
    "    NUM_CLASSES = 4  # Updated for 4 classes\n",
    "    \n",
    "    SEED = 42\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create output directory for results\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  - Target classes: {config.TARGET_CLASSES}\")\n",
    "print(f\"  - Number of classes: {config.NUM_CLASSES}\")\n",
    "print(f\"  - Image size: {config.IMG_SIZE_MOBILE}x{config.IMG_SIZE_MOBILE}\")\n",
    "print(f\"  - Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  - Output directory: {config.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22624fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 2. DATASET FILTERING & PREPARATION ------------------\n",
    "\n",
    "def filter_dataset_classes(source_dir, dest_dir, target_classes):\n",
    "    \"\"\"\n",
    "    Copy only the target classes from source to destination.\n",
    "    Preserves train/test directory structure.\n",
    "    \"\"\"\n",
    "    for split in ['train', 'test']:\n",
    "        source_split = os.path.join(source_dir, split)\n",
    "        dest_split = os.path.join(dest_dir, split)\n",
    "        \n",
    "        if not os.path.exists(source_split):\n",
    "            continue\n",
    "            \n",
    "        for class_name in target_classes:\n",
    "            source_class = os.path.join(source_split, class_name)\n",
    "            dest_class = os.path.join(dest_split, class_name)\n",
    "            \n",
    "            if os.path.exists(source_class):\n",
    "                os.makedirs(dest_class, exist_ok=True)\n",
    "                if not os.listdir(dest_class):  # Only copy if destination is empty\n",
    "                    shutil.copytree(source_class, dest_class, dirs_exist_ok=True)\n",
    "                    print(f\"  Copied {class_name} from {split}/\")\n",
    "    \n",
    "    print(f\"✓ Dataset filtered to {len(target_classes)} classes\")\n",
    "\n",
    "# Copy and filter dataset\n",
    "if not os.path.exists(config.WORK_DIR):\n",
    "    print(\"Filtering and copying dataset to working directory...\")\n",
    "    filter_dataset_classes(config.BASE_INPUT, config.WORK_DIR, config.TARGET_CLASSES)\n",
    "else:\n",
    "    print(\"✓ Working directory already exists\")\n",
    "\n",
    "TRAIN_DIR = f'{config.WORK_DIR}/train'\n",
    "TEST_DIR = f'{config.WORK_DIR}/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61bd5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 3. DATA SPLITTING & GENERATORS ------------------\n",
    "\n",
    "def create_data_generators(train_dir, test_dir, train_ratio, img_size, batch_size):\n",
    "    \"\"\"\n",
    "    Create train/validation data generators with specified split ratio.\n",
    "    Merges train+test, then splits based on train_ratio.\n",
    "    \n",
    "    Args:\n",
    "        train_dir: Original training directory\n",
    "        test_dir: Original test directory (will be merged with train)\n",
    "        train_ratio: Ratio for training (e.g., 0.9 for 90:10 split)\n",
    "        img_size: Target image size\n",
    "        batch_size: Batch size for generators\n",
    "    \n",
    "    Returns:\n",
    "        train_generator, val_generator, class_indices\n",
    "    \"\"\"\n",
    "    # Only rescale, no augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=1.0 - train_ratio  # validation_split is the % for validation\n",
    "    )\n",
    "    \n",
    "    # Use train directory for both train and validation (with validation_split)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        shuffle=True,\n",
    "        seed=SEED\n",
    "    )\n",
    "    \n",
    "    val_generator = datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        shuffle=False,\n",
    "        seed=SEED\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator, train_generator.class_indices\n",
    "\n",
    "print(\"✓ Data generator function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4330a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 4. DEFINE 18 TRAINING SCENARIOS ------------------\n",
    "\n",
    "# Based on the table: 3 scenarios × 2 optimizers × 3 learning rates = 18 experiments\n",
    "scenarios = []\n",
    "\n",
    "scenario_configs = [\n",
    "    # Scenario 1: 90:10 split\n",
    "    {'split_ratio': 0.90, 'split_name': '90:10', 'lr': 0.1, 'epochs': 15},\n",
    "    {'split_ratio': 0.90, 'split_name': '90:10', 'lr': 0.01, 'epochs': 30},\n",
    "    {'split_ratio': 0.90, 'split_name': '90:10', 'lr': 0.001, 'epochs': 45},\n",
    "    \n",
    "    # Scenario 2: 80:20 split\n",
    "    {'split_ratio': 0.80, 'split_name': '80:20', 'lr': 0.1, 'epochs': 15},\n",
    "    {'split_ratio': 0.80, 'split_name': '80:20', 'lr': 0.01, 'epochs': 30},\n",
    "    {'split_ratio': 0.80, 'split_name': '80:20', 'lr': 0.001, 'epochs': 45},\n",
    "    \n",
    "    # Scenario 3: 70:30 split\n",
    "    {'split_ratio': 0.70, 'split_name': '70:30', 'lr': 0.1, 'epochs': 15},\n",
    "    {'split_ratio': 0.70, 'split_name': '70:30', 'lr': 0.01, 'epochs': 30},\n",
    "    {'split_ratio': 0.70, 'split_name': '70:30', 'lr': 0.001, 'epochs': 45},\n",
    "]\n",
    "\n",
    "optimizers_list = ['Adam', 'SGD']\n",
    "\n",
    "# Generate all 18 combinations\n",
    "scenario_id = 1\n",
    "for sc in scenario_configs:\n",
    "    for opt in optimizers_list:\n",
    "        scenarios.append({\n",
    "            'id': scenario_id,\n",
    "            'split_ratio': sc['split_ratio'],\n",
    "            'split_name': sc['split_name'],\n",
    "            'optimizer': opt,\n",
    "            'learning_rate': sc['lr'],\n",
    "            'epochs': sc['epochs']\n",
    "        })\n",
    "        scenario_id += 1\n",
    "\n",
    "print(f\"✓ Created {len(scenarios)} training scenarios\")\n",
    "print(\"\\nScenario Summary:\")\n",
    "for i, s in enumerate(scenarios[:3], 1):\n",
    "    print(f\"  Scenario {s['id']}: {s['split_name']} split, {s['optimizer']}, LR={s['learning_rate']}, Epochs={s['epochs']}\")\n",
    "print(f\"  ... and {len(scenarios)-3} more scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f087a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 5. BUILD FEATURE EXTRACTION MODEL ------------------\n",
    "\n",
    "def build_feature_extraction_model(num_classes, img_size, optimizer_name, learning_rate, dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    Build MobileNetV3Large model for feature extraction (frozen base).\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of output classes\n",
    "        img_size: Input image size\n",
    "        optimizer_name: 'Adam' or 'SGD'\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        dropout_rate: Dropout rate before output layer\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Load pre-trained MobileNetV3Large without top layers\n",
    "    base_model = MobileNetV3Large(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(img_size, img_size, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze all layers in base model (feature extraction)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Build custom classification head\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ])\n",
    "    \n",
    "    # Select optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        opt = optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Model builder function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85629957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 6. TRAINING LOOP FOR ALL SCENARIOS ------------------\n",
    "\n",
    "results = []\n",
    "best_val_acc = float(\"-inf\")\n",
    "best_model_path = None\n",
    "best_scenario_id = None\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"STARTING TRAINING FOR {len(scenarios)} SCENARIOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SCENARIO {scenario['id']}/{len(scenarios)}\")\n",
    "    print(f\"  Split: {scenario['split_name']}\")\n",
    "    print(f\"  Optimizer: {scenario['optimizer']}\")\n",
    "    print(f\"  Learning Rate: {scenario['learning_rate']}\")\n",
    "    print(f\"  Epochs: {scenario['epochs']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Create data generators for this split ratio\n",
    "        train_gen, val_gen, class_indices = create_data_generators(\n",
    "            TRAIN_DIR,\n",
    "            TEST_DIR,\n",
    "            train_ratio=scenario['split_ratio'],\n",
    "            img_size=config.IMG_SIZE_MOBILE,\n",
    "            batch_size=config.BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        print(f\"  Training samples: {train_gen.samples}\")\n",
    "        print(f\"  Validation samples: {val_gen.samples}\")\n",
    "        \n",
    "        # Build model\n",
    "        model = build_feature_extraction_model(\n",
    "            num_classes=config.NUM_CLASSES,\n",
    "            img_size=config.IMG_SIZE_MOBILE,\n",
    "            optimizer_name=scenario['optimizer'],\n",
    "            learning_rate=scenario['learning_rate'],\n",
    "            dropout_rate=config.DROPOUT_RATE\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stop = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=scenario['epochs'],\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Extract final metrics\n",
    "        final_train_acc = history.history['accuracy'][-1]\n",
    "        final_train_loss = history.history['loss'][-1]\n",
    "        final_val_acc = history.history['val_accuracy'][-1]\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'scenario_id': scenario['id'],\n",
    "            'split_ratio': scenario['split_name'],\n",
    "            'optimizer': scenario['optimizer'],\n",
    "            'learning_rate': scenario['learning_rate'],\n",
    "            'epochs': scenario['epochs'],\n",
    "            'train_accuracy': final_train_acc,\n",
    "            'train_loss': final_train_loss,\n",
    "            'val_accuracy': final_val_acc,\n",
    "            'val_loss': final_val_loss,\n",
    "            'history': history.history\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"\\n  ✓ Scenario {scenario['id']} completed!\")\n",
    "        print(f\"    Final Train Accuracy: {final_train_acc:.4f}\")\n",
    "        print(f\"    Final Val Accuracy: {final_val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model as training progresses\n",
    "        if final_val_acc > best_val_acc:\n",
    "            best_val_acc = final_val_acc\n",
    "            best_scenario_id = scenario['id']\n",
    "            best_model_path = os.path.join(config.OUTPUT_DIR, 'best_model.h5')\n",
    "            model.save(best_model_path)\n",
    "            print(f\"    ✓ New best model saved: {best_model_path}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del model\n",
    "        del train_gen\n",
    "        del val_gen\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ✗ Scenario {scenario['id']} failed: {str(e)}\")\n",
    "        results.append({\n",
    "            'scenario_id': scenario['id'],\n",
    "            'split_ratio': scenario['split_name'],\n",
    "            'optimizer': scenario['optimizer'],\n",
    "            'learning_rate': scenario['learning_rate'],\n",
    "            'epochs': scenario['epochs'],\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TRAINING COMPLETED: {len([r for r in results if 'val_accuracy' in r])}/{len(scenarios)} scenarios successful\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb092b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 7. PREPARE RESULTS DATAFRAME ------------------\n",
    "\n",
    "# Filter successful results\n",
    "successful_results = [r for r in results if 'val_accuracy' in r]\n",
    "\n",
    "if len(successful_results) == 0:\n",
    "    print(\"⚠ No successful training runs to analyze!\")\n",
    "else:\n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame([{\n",
    "        'Scenario_ID': r['scenario_id'],\n",
    "        'Split_Ratio': r['split_ratio'],\n",
    "        'Optimizer': r['optimizer'],\n",
    "        'Learning_Rate': r['learning_rate'],\n",
    "        'Epochs': r['epochs'],\n",
    "        'Train_Accuracy': r['train_accuracy'],\n",
    "        'Val_Accuracy': r['val_accuracy'],\n",
    "        'Train_Loss': r['train_loss'],\n",
    "        'Val_Loss': r['val_loss']\n",
    "    } for r in successful_results])\n",
    "    \n",
    "    # Sort by validation accuracy\n",
    "    df_results = df_results.sort_values('Val_Accuracy', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_csv = os.path.join(config.OUTPUT_DIR, 'scenario_results.csv')\n",
    "    df_results.to_csv(results_csv, index=False)\n",
    "    \n",
    "    # Save detailed history to JSON\n",
    "    history_json = os.path.join(config.OUTPUT_DIR, 'training_history.json')\n",
    "    with open(history_json, 'w') as f:\n",
    "        json.dump(successful_results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n✓ Results saved:\")\n",
    "    print(f\"  - CSV: {results_csv}\")\n",
    "    print(f\"  - JSON: {history_json}\")\n",
    "    \n",
    "    # Display top 10\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TOP 10 SCENARIOS BY VALIDATION ACCURACY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(df_results.head(10).to_string(index=False))\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 8. VISUALIZATION: TOP 10 BAR CHART ------------------\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    # Get top 10 scenarios\n",
    "    top_10 = df_results.head(10).copy()\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create labels with all info\n",
    "    top_10['Label'] = top_10.apply(\n",
    "        lambda x: f\"S{x['Scenario_ID']}: {x['Split_Ratio']}, {x['Optimizer']}, LR={x['Learning_Rate']}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    bars = plt.barh(range(len(top_10)), top_10['Val_Accuracy'], color='steelblue', alpha=0.8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, top_10['Val_Accuracy'])):\n",
    "        plt.text(val + 0.005, i, f'{val:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.yticks(range(len(top_10)), top_10['Label'])\n",
    "    plt.xlabel('Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "    plt.title('Top 10 Scenarios by Validation Accuracy', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_path = os.path.join(config.OUTPUT_DIR, 'top_10_scenarios.png')\n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Top 10 bar chart saved: {fig_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠ No results to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93870539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 9. VISUALIZATION: TOP 3 TRAINING CURVES ------------------\n",
    "\n",
    "if len(successful_results) >= 3:\n",
    "    top_3_ids = df_results.head(3)['Scenario_ID'].tolist()\n",
    "    top_3_results = [r for r in successful_results if r['scenario_id'] in top_3_ids]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for idx, result in enumerate(top_3_results):\n",
    "        ax = axes[idx]\n",
    "        history = result['history']\n",
    "        epochs_range = range(1, len(history['accuracy']) + 1)\n",
    "        \n",
    "        # Plot accuracy\n",
    "        ax.plot(epochs_range, history['accuracy'], 'b-', label='Train Accuracy', linewidth=2)\n",
    "        ax.plot(epochs_range, history['val_accuracy'], 'r-', label='Val Accuracy', linewidth=2)\n",
    "        \n",
    "        ax.set_title(\n",
    "            f\"Scenario {result['scenario_id']}: {result['split_ratio']}\\n\"\n",
    "            f\"{result['optimizer']}, LR={result['learning_rate']}, Val Acc={result['val_accuracy']:.4f}\",\n",
    "            fontsize=11, fontweight='bold'\n",
    "        )\n",
    "        ax.set_xlabel('Epoch', fontsize=10)\n",
    "        ax.set_ylabel('Accuracy', fontsize=10)\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_path = os.path.join(config.OUTPUT_DIR, 'top_3_training_curves.png')\n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Top 3 training curves saved: {fig_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠ Not enough results to plot top 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea32bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 10. VISUALIZATION: CONFUSION MATRIX FOR BEST SCENARIO ------------------\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    # Get best scenario\n",
    "    best_scenario_id = df_results.iloc[0]['Scenario_ID']\n",
    "    best_result = [r for r in successful_results if r['scenario_id'] == best_scenario_id][0]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING BEST SCENARIO: {best_scenario_id}\")\n",
    "    print(f\"  Split: {best_result['split_ratio']}\")\n",
    "    print(f\"  Optimizer: {best_result['optimizer']}\")\n",
    "    print(f\"  Learning Rate: {best_result['learning_rate']}\")\n",
    "    print(f\"  Val Accuracy: {best_result['val_accuracy']:.4f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if isinstance(best_result['split_ratio'], str):\n",
    "        split_num = float(best_result['split_ratio'].split(':')[0]) / 100.0\n",
    "    else:\n",
    "        split_num = best_result['split_ratio']\n",
    "    \n",
    "    # Recreate data generators for best scenario\n",
    "    train_gen, val_gen, class_indices = create_data_generators(\n",
    "        TRAIN_DIR,\n",
    "        TEST_DIR,\n",
    "        train_ratio=split_num,\n",
    "        img_size=config.IMG_SIZE_MOBILE,\n",
    "        batch_size=config.BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # Load best model if available; otherwise retrain and save\n",
    "    if best_model_path and os.path.exists(best_model_path):\n",
    "        best_model = tf.keras.models.load_model(best_model_path)\n",
    "        print(f\"\\n✓ Loaded best model from: {best_model_path}\")\n",
    "    else:\n",
    "        best_model = build_feature_extraction_model(\n",
    "            num_classes=config.NUM_CLASSES,\n",
    "            img_size=config.IMG_SIZE_MOBILE,\n",
    "            optimizer_name=best_result['optimizer'],\n",
    "            learning_rate=best_result['learning_rate'],\n",
    "            dropout_rate=config.DROPOUT_RATE\n",
    "        )\n",
    "        \n",
    "        early_stop = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        best_model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=best_result['epochs'],\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        best_model_path = os.path.join(config.OUTPUT_DIR, 'best_model.h5')\n",
    "        best_model.save(best_model_path)\n",
    "        print(f\"\\n✓ Best model saved: {best_model_path}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    val_gen.reset()\n",
    "    y_pred = best_model.predict(val_gen, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = val_gen.classes[:len(y_pred_classes)]\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = list(class_indices.keys())\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(f'Confusion Matrix - Best Scenario {best_scenario_id}\\n'\n",
    "              f'{best_result[\"split_ratio\"]}, {best_result[\"optimizer\"]}, LR={best_result[\"learning_rate\"]}',\n",
    "              fontsize=13, fontweight='bold', pad=15)\n",
    "    plt.ylabel('True Label', fontsize=11, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontsize=11, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_path = os.path.join(config.OUTPUT_DIR, 'best_scenario_confusion_matrix.png')\n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Confusion matrix saved: {fig_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred_classes, target_names=class_names))\n",
    "    \n",
    "    # Cleanup\n",
    "    del best_model\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No results to evaluate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 11. FINAL SUMMARY ------------------\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total Scenarios: {len(scenarios)}\")\n",
    "    print(f\"Successful Runs: {len(successful_results)}\")\n",
    "    print(f\"Failed Runs: {len(scenarios) - len(successful_results)}\")\n",
    "    print(f\"\\nBest Scenario: {df_results.iloc[0]['Scenario_ID']}\")\n",
    "    print(f\"  Configuration: {df_results.iloc[0]['Split_Ratio']}, \"\n",
    "          f\"{df_results.iloc[0]['Optimizer']}, LR={df_results.iloc[0]['Learning_Rate']}\")\n",
    "    print(f\"  Validation Accuracy: {df_results.iloc[0]['Val_Accuracy']:.4f}\")\n",
    "    print(f\"\\nWorst Scenario: {df_results.iloc[-1]['Scenario_ID']}\")\n",
    "    print(f\"  Configuration: {df_results.iloc[-1]['Split_Ratio']}, \"\n",
    "          f\"{df_results.iloc[-1]['Optimizer']}, LR={df_results.iloc[-1]['Learning_Rate']}\")\n",
    "    print(f\"  Validation Accuracy: {df_results.iloc[-1]['Val_Accuracy']:.4f}\")\n",
    "    \n",
    "    # Performance comparison by optimizer\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PERFORMANCE BY OPTIMIZER\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for opt in ['Adam', 'SGD']:\n",
    "        opt_results = df_results[df_results['Optimizer'] == opt]\n",
    "        if len(opt_results) > 0:\n",
    "            print(f\"{opt}:\")\n",
    "            print(f\"  Mean Val Accuracy: {opt_results['Val_Accuracy'].mean():.4f}\")\n",
    "            print(f\"  Best Val Accuracy: {opt_results['Val_Accuracy'].max():.4f}\")\n",
    "            print(f\"  Worst Val Accuracy: {opt_results['Val_Accuracy'].min():.4f}\")\n",
    "    \n",
    "    # Performance comparison by split ratio\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PERFORMANCE BY SPLIT RATIO\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for split in ['90:10', '80:20', '70:30']:\n",
    "        split_results = df_results[df_results['Split_Ratio'] == split]\n",
    "        if len(split_results) > 0:\n",
    "            print(f\"{split}:\")\n",
    "            print(f\"  Mean Val Accuracy: {split_results['Val_Accuracy'].mean():.4f}\")\n",
    "            print(f\"  Best Val Accuracy: {split_results['Val_Accuracy'].max():.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"All outputs saved to:\", config.OUTPUT_DIR)\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"\\n✓ Multi-scenario transfer learning experiment completed successfully!\")\n",
    "else:\n",
    "    print(\"\\n⚠ No successful results to summarize\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
